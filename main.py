# Written by Anmol Gulati

import string
import re
from collections import Counter
from itertools import product
from itertools import permutations
import random
from numpy import cumsum
import numpy as np

# adjust on your own
P_my = 0.77
P_fake = 0.23
num_charactors = 1000

with open("script.txt", encoding="utf-8") as f:
    data = f.read()


def process_text(data):
    ''' Preprocess the text data '''
    data = data.lower()
    data = re.sub(r"[^a-z ]+", "", data)
    data = " ".join(data.split())
    return data


data = process_text(data)

# all possible characters
allchar = " " + string.ascii_lowercase

# unigram
unigram = Counter(data)
unigram_prob = {ch: round(unigram[ch] / len(data), 4) for ch in allchar}
uni_list = [unigram_prob[c] for c in allchar]

# to distinguish between fake_unigram_prob below
my_unigram_prob = unigram_prob


def ngram(n, data=data):
    ''' Generate n-gram '''
    # all possible n-grams
    d = dict.fromkeys(["".join(i) for i in product(allchar, repeat=n)], 0)
    # update counts
    d.update(Counter(data[x: x + n] for x in range(len(data) - 1)))
    return d


# bigram
bigram = ngram(2)
bigram_prob = {c: bigram[c] / unigram[c[0]] for c in bigram}
bigram_prob_L = {c: (bigram[c] + 1) / (unigram[c[0]] + 27) for c in bigram}

# trigram
trigram = ngram(3)
trigram_prob_L = {c: (trigram[c] + 1) / (bigram[c[:2]] + 27) for c in trigram}


# based on https://python-course.eu/numerical-programming/weighted-probabilities.php
def weighted_choice(collection, weights):
    """Randomly choose an element from collection according to weights"""
    weights = np.array(weights)
    weights_sum = weights.sum()
    weights = weights.cumsum() / weights_sum
    x = random.random()
    for i in range(len(weights)):
        if x < weights[i]:
            return collection[i]


def gen_bi(c):
    ''' Generate the second char '''
    w = [bigram_prob[c + i] for i in allchar]
    return weighted_choice(allchar, weights=w)[0]


def gen_tri(ab):
    ''' Generate the third char '''
    w = [trigram_prob_L[ab + i] for i in allchar]
    return weighted_choice(allchar, weights=w)[0]


def gen_sen(c, num):
    ''' generate the second char'''
    res = c + gen_bi(c)
    for i in range(num - 2):
        if bigram[res[-2:]] == 0:
            t = gen_bi(res[-1])
        else:
            t = gen_tri(res[-2:])
        res += t
    return res


# generate sentences
sentences = []
for char in allchar:
    sentence = gen_sen(char, num_charactors)
    sentences.append(sentence)

## fake script
with open("fake_script.txt", encoding="utf-8") as f:
    fake_data = f.read()
fake_data = process_text(fake_data)

unigram = Counter(data)
unigram_prob = {ch: round(unigram[ch] / len(data), 4) for ch in allchar}
uni_list = [unigram_prob[c] for c in allchar]

fake_unigram_prob = unigram_prob

count = 0
for char in allchar:
    count += 1
    print(
        P_fake
        * fake_unigram_prob[char]
        / (P_fake * fake_unigram_prob[char] + P_my * my_unigram_prob[char])
    )

# 1. Script chosen was "Joker"

# 2. Unigram Probabilities
unigram = ngram(1)  # unigram transition probability table
unigram_prob = {ch: round(unigram[ch] / len(data), 4) for ch in unigram}
# write the unigram probabilities (27 numbers) to a file (comma separated)
with open("unigram_prob.txt", "w") as f:
    f.write(",".join([str(unigram_prob[ch]) for ch in allchar]))

# 3. Bigram transition probabilities without Laplace smoothing
bigram = ngram(2)
bigram_prob = {ch: round(bigram[ch] / unigram[ch[0]], 4) for ch in bigram}
with open("bigram_prob_without_laplace.txt", "w") as f:
    for ch1 in allchar:
        f.write(",".join([str(bigram_prob[ch1 + ch2]) for ch2 in allchar]) + "\n")

# 4. Bigram transition probabilities with Laplace smoothing
bigram_prob_laplace = {ch: round((bigram[ch] + 1) / (unigram[ch[0]] + 27), 5) for ch in bigram}  # with laplace smoothing
with open("bigram_prob_with_laplace.txt", "w") as f:
    for ch1 in allchar:
        f.write(",".join([str(bigram_prob_laplace[ch1 + ch2]) for ch2 in allchar]) + "\n")

# 5. 26 sentences generated by trigram and bigram models
sentences = []
with open("sentences.txt", "w") as f:
    for ch in string.ascii_lowercase:
        sentence = gen_sen(ch, 1000)
        f.write(sentence + "\n")
        sentences.append(sentence)

# 6. An example of a sentence generated by the model
print(gen_sen("a", 1000))

# 7. likelihood probabilities of the Naive Bayes estimator for the fake script
fake_unigram = ngram(1, data=fake_data)
fake_unigram_prob = {ch: round(fake_unigram[ch] / len(fake_data), 4) for ch in fake_unigram}
with open('fake_likelihood.txt', 'w') as f: # write the fake unigram probabilities (27 numbers) to a file (comma separated)
    f.write(','.join([str(fake_unigram_prob[ch]) for ch in allchar]))

# 8. posterior probabilities of the Naive Bayes estimator for the fake script
with open('fake_posterior.txt', 'w') as f:
    f.write(','.join(
        [str(round(P_fake * fake_unigram_prob[ch] / (P_fake * fake_unigram_prob[ch] + P_my * unigram_prob[ch]), 4)) for
         ch in allchar]))

# 9.  Naive Bayes model to predict which document the 26 sentences your generated in Question 5 came from
with open('classification.txt', 'w') as f:
    classifications = []
    for sentence in sentences:
        fake = np.log10(P_fake)
        real = np.log10(P_my)
        for ch in sentence:
            real += np.log10(unigram_prob[ch])
            fake += np.log10(fake_unigram_prob[ch])
        if real > fake: # classify the sentences generated in question 5 as real or fake
            classifications.append('0')
        else:
            classifications.append('1')

    f.write(','.join(classifications))
